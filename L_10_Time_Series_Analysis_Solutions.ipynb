{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "How can we interpret the plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/vhaus63/ids_data/main/acf_exercise.png' style='width:40%; float:left;' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x axis represents the lag values (i.e., how far back in time you are looking). Lag 0 represents the correlation of the series with itself. Therefore this bar represents perfect correlation of value 1. Going further right on the x axis, we can see that the current value of the timeseries is correlated negatively with its prior value. Bars outside the blue bounds (95% confidence interval) are statistically significant. This means, that their correlation is unlikely to be resulting from random chance.\n",
    "\n",
    "In this example, we have another significant negative correlation with the value at lag=3 and a positive correlation at lag=4.\n",
    "\n",
    "In general, the ACF decays quickly as the lag increases. This supports the assumption, that the differenced timeseries has become stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Why do we use the \"raw\" and not the already differenciated timeseries `temp_global_training`? Could we also use the `temp_global_training_diff1` timeseries? If so, which approach is better and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we used `temp_global_training_diff1` with the ARIMA(3,1,0) model, we would differentiate two times overall (once already before using the model and then again in the model since the parameter d is set to 1). This would not make sense here since we know already from our stationarity check that first order differencing is enough.\n",
    "\n",
    "So overall we would have two options:\n",
    "1. We differenciate \"by hand\" at first and then use an ARMA(p,q) model with the differenciated time series `temp_global_training_diff1` (which is the same as an ARIMA(p,0,q) model)\n",
    "2. We do not differentiate the original time series in the beginning, but put it in an ARIMA(p,1,q) model.\n",
    "In general, it does not play a role whether we use the first or the second approach from the modelling point of view. However, keep in mind that we want to forcast values into the future. If we used the first approach, the model would provide a fitted time series based on the differenced values, which would not have a direct real-world interpretation. To obtain meaningful values, we would need to reverse the differencing transformation. In contrast, the second approach would return the original values directly, without requiring any additional transformation.\n",
    "\n",
    "Therefore, the second approach should be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Fit some variations of ARIMA(p, d, q) models including ARIMA(3, 1, 1), ARIMA(3, 1, 2), ARIMA(2, 1, 2) and compare their AICc. Which model should be chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this code into the original notebook from the lecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ARIMA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mARIMA\u001b[49m(temp_global_training, order\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      2\u001b[0m model_fit \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mARIMA(3,1,0) - AICc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(model_fit\u001b[38;5;241m.\u001b[39maicc,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ARIMA' is not defined"
     ]
    }
   ],
   "source": [
    "model = ARIMA(temp_global_training, order=(3, 1, 0))\n",
    "model_fit = model.fit()\n",
    "print(f\"ARIMA(3,1,0) - AICc: {round(model_fit.aicc,2)}\")\n",
    "\n",
    "model = ARIMA(temp_global_training, order=(3, 1, 1))\n",
    "model_fit = model.fit()\n",
    "print(f\"ARIMA(3,1,1) - AICc: {round(model_fit.aicc,2)}\")\n",
    "\n",
    "model = ARIMA(temp_global_training, order=(3, 1, 2))\n",
    "model_fit = model.fit()\n",
    "print(f\"ARIMA(3,1,2) - AICc: {round(model_fit.aicc,2)}\")\n",
    "\n",
    "model = ARIMA(temp_global_training, order=(2, 1, 2))\n",
    "model_fit = model.fit()\n",
    "print(f\"ARIMA(2,1,2) - AICc: {round(model_fit.aicc,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ARIMA(2,1,2) has the smallest AICc and should thus be preferred."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
